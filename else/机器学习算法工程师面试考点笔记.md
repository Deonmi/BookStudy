# 机器学习/算法工程师面试考点笔记

问题链接：[牛客网](<https://www.nowcoder.com/tutorial/95/a785d36cf4264dfa93c3de133d0bb339>)

## 1. 数学基础

### 微积分

#### 1. SGD，Momentum，Adagard，Adam原理

**答案：**

> - SGD为随机梯度下降，每一次迭代计算数据集的mini-batch的梯度，然后对参数进行跟新。
>
> - Momentum参考了物理中动量的概念，前几次的梯度也会参与到当前的计算中，但是前几轮的梯度叠加在当前计算中会有一定的衰减。
>
> - Adagard在训练的过程中可以自动变更学习的速率，设置一个全局的学习率，而实际的学习率与以往的参数模和的开方成反比。
>
> - Adam利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率，在经过偏置的校正后，每一次迭代后的学习率都有个确定的范围，使得参数较为平稳。

**参考链接：**

[深入浅出--梯度下降法及其实现](<https://www.jianshu.com/p/c7e642877b0e>)

[深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）](https://www.cnblogs.com/guoyaohua/p/8542554.html)

**笔记：**

**1. 梯度下降**

梯度下降公式：

![img](https://upload-images.jianshu.io/upload_images/1234352-f20521a962005299.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

手打公式学习下：

$\Theta^1 = \Theta^0 - \alpha \bigtriangledown J(\Theta)$	evaluated at $\Theta^0$

均方误差代价函数：

![img](https://upload-images.jianshu.io/upload_images/1234352-4e4000e69f05af7b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/451/format/webp)

手打公式学习下：

$J(\Theta) = \frac 1 {2m} 
\sum\limits_{i=1}^m (h_\Theta (x ^ {(i)} ) - y ^ {(i)} ) ^ 2 $

> 这个下山的人实际上就代表了反向传播算法，下山的路径其实就代表着算法中一直在寻找的参数Θ，山上当前点的最陡峭的方向实际上就是代价函数在这一点的梯度方向，场景中观测最陡峭方向所用的工具就是微分 。在下一次观测之前的时间就是有我们算法中的学习率α所定义的。