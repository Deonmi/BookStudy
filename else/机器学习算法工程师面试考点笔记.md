# 机器学习/算法工程师面试考点笔记

问题链接：[牛客网](<https://www.nowcoder.com/tutorial/95/a785d36cf4264dfa93c3de133d0bb339>)

复习思维导图：

![img](https://uploadfiles.nowcoder.com/images/20190320/826546_1553063448688_ED04C8DA77670800ECFD8C16908B7BC5)

## 1. 数学基础

### 微积分

#### 1. SGD，Momentum，Adagard，Adam原理

> - SGD为随机梯度下降，每一次迭代计算数据集的mini-batch的梯度，然后对参数进行跟新。
>
> - Momentum参考了物理中动量的概念，前几次的梯度也会参与到当前的计算中，但是前几轮的梯度叠加在当前计算中会有一定的衰减。
>
> - Adagard在训练的过程中可以自动变更学习的速率，设置一个全局的学习率，而实际的学习率与以往的参数模和的开方成反比。
>
> - Adam利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率，在经过偏置的校正后，每一次迭代后的学习率都有个确定的范围，使得参数较为平稳。

- 参考链接：

[深入浅出--梯度下降法及其实现](<https://www.jianshu.com/p/c7e642877b0e>)

[深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）](https://www.cnblogs.com/guoyaohua/p/8542554.html)

- 梯度下降

梯度下降公式：

![img](https://upload-images.jianshu.io/upload_images/1234352-f20521a962005299.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

手打公式学习下：

$\Theta^1 = \Theta^0 - \alpha \bigtriangledown J(\Theta)$	evaluated at $\Theta^0$

均方误差代价函数：

![img](https://upload-images.jianshu.io/upload_images/1234352-4e4000e69f05af7b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/451/format/webp)

手打公式学习下：

$J(\Theta) = \frac 1 {2m} 
\sum\limits_{i=1}^m (h_\Theta (x ^ {(i)} ) - y ^ {(i)} ) ^ 2 $

这个下山的人实际上就代表了反向传播算法，下山的路径其实就代表着算法中一直在寻找的参数Θ，山上当前点的最陡峭的方向实际上就是代价函数在这一点的梯度方向，场景中观测最陡峭方向所用的工具就是微分 。在下一次观测之前的时间就是有我们算法中的学习率α所定义的。

#### 2. L1不可导的时候该怎么办

> 当损失函数不可导,梯度下降不再有效,可以使用坐标轴下降法,梯度下降是沿着当前点的负梯度方向进行参数更新,而坐标轴下降法是沿着坐标轴的方向,假设有m个特征个数,坐标轴下降法进参数更新的时候,先固定m-1个值,然后再求另外一个的局部最优解,从而避免损失函数不可导问题。
>
> 使用Proximal Algorithm对L1进行求解,此方法是去优化损失函数上界结果。

#### 3. sigmoid函数特性

> 定义域为$(-\infty, +\infty)$，值域为$(-1, 1)$，函数在定义域内为连续光滑的函数处处可导，导数为$f^{'}(x) = f(x)(1-f(x))$。

### 统计学概率论

#### 1. 一个活动,n个女生手里拿着长短不一的玫瑰花,无序的排成一排,一个男生从头走到尾,试图拿更长的玫瑰花,一旦拿了一朵就不能再拿其他的,错过了就不能回头,问最好的策略?

