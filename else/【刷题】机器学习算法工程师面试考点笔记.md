# 机器学习/算法工程师面试考点笔记

[TOC]

问题链接：[牛客网](<https://www.nowcoder.com/tutorial/95/a785d36cf4264dfa93c3de133d0bb339>)

复习思维导图：

![img](https://uploadfiles.nowcoder.com/images/20190320/826546_1553063448688_ED04C8DA77670800ECFD8C16908B7BC5)

## 1. 数学基础

### 微积分

#### 1. SGD，Momentum，Adagard，Adam原理

> - SGD为随机梯度下降，每一次迭代计算数据集的mini-batch的梯度，然后对参数进行跟新。
>
> - Momentum参考了物理中动量的概念，前几次的梯度也会参与到当前的计算中，但是前几轮的梯度叠加在当前计算中会有一定的衰减。
>
> - Adagard在训练的过程中可以自动变更学习的速率，设置一个全局的学习率，而实际的学习率与以往的参数模和的开方成反比。
>
> - Adam利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率，在经过偏置的校正后，每一次迭代后的学习率都有个确定的范围，使得参数较为平稳。

- 参考链接：

[深入浅出--梯度下降法及其实现](<https://www.jianshu.com/p/c7e642877b0e>)

[深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）](https://www.cnblogs.com/guoyaohua/p/8542554.html)

- 梯度下降

梯度下降公式：

![img](https://upload-images.jianshu.io/upload_images/1234352-f20521a962005299.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

手打公式学习下：

$\Theta^1 = \Theta^0 - \alpha \bigtriangledown J(\Theta)$	evaluated at $\Theta^0$

均方误差代价函数：

![img](https://upload-images.jianshu.io/upload_images/1234352-4e4000e69f05af7b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/451/format/webp)

手打公式学习下：

$J(\Theta) = \frac 1 {2m} 
\sum\limits_{i=1}^m (h_\Theta (x ^ {(i)} ) - y ^ {(i)} ) ^ 2 $

这个下山的人实际上就代表了反向传播算法，下山的路径其实就代表着算法中一直在寻找的参数Θ，山上当前点的最陡峭的方向实际上就是代价函数在这一点的梯度方向，场景中观测最陡峭方向所用的工具就是微分 。在下一次观测之前的时间就是有我们算法中的学习率α所定义的。

#### 2. L1不可导的时候该怎么办

> 当损失函数不可导，梯度下降不再有效，可以使用坐标轴下降法，梯度下降是沿着当前点的负梯度方向进行参数更新，而坐标轴下降法是沿着坐标轴的方向，假设有m个特征个数，坐标轴下降法进参数更新的时候，先固定m-1个值，然后再求另外一个的局部最优解，从而避免损失函数不可导问题。
>
> 使用Proximal Algorithm对L1进行求解，此方法是去优化损失函数上界结果。

#### 3. sigmoid函数特性

> 定义域为$(-\infty， +\infty)$，值域为$(-1， 1)$，函数在定义域内为连续光滑的函数处处可导，导数为$f^{'}(x) = f(x)(1-f(x))$。

### 统计学概率论

#### 1. 一个活动，n个女生手里拿着长短不一的玫瑰花，无序的排成一排，一个男生从头走到尾，试图拿更长的玫瑰花，一旦拿了一朵就不能再拿其他的，错过了就不能回头，问最好的策略?

> 选择的策略为不选取前r-1个女生，只从剩下的n-r+1个女生开始选取，若任何一个女生比之前的女生玫瑰花都长则选取这个女生，假设从第r个女生开始选，则第k个被选中的女生拥有最长玫瑰花的概率为：
>
> $P=\sum\limits_{k=r}^n P(第k个女生被选中且拥有最常的玫瑰花)$
>
> ​	$=\sum\limits_{k=r}^n P(第k个女生的玫瑰最长)P(第k个女生被选中|第k个女生的玫瑰最长)$
>
> ​	$=\sum\limits_{k=r}^n \frac{1}{n} P(拥有最常玫瑰的女生出现在前r-1个女生中)$
>
> ​	$=\sum\limits_{k=r}^n \frac{1}{n} \frac{r-1}{k-1}$
>
> ​	$=\frac{r-1}{n} \sum\limits_{k=r}^{n} \frac{1}{k-1}$
>
> 当第r个为玫瑰最长的女生，那么她被选中概率比第r+1个女生大，则
>
> $P_{r+1} \leq P_r$
>
> 即
>
> $r^* = min\{ r | \sum_{k=r+1}^n \frac{1}{k-1} \leq 1 \}$
>
> 因
>
> $\sum_{k=r+1}^n \frac{1}{k-1} \approx ln(\frac{n}{r^*} = 1， r^* = n/e)$
>
> 所以
>
> $P_{r^*} = 1/e \approx 0.368$
>
> 在此策略下，玫瑰最长女生被选中的概率为0.368

- 衍生，参考文献：[37法则，一种无奈的策略](https://baijiahao.baidu.com/s?id=1621436361074205070&wfr=spider&for=pc)

37法则是指，在统计学中，对样本数量固定（每个样本只出现一次）的情况下，面对选择其中最优的那个样本这个问题时，如何保证选择的样本尽可能最优的一种策略。就是**把样本总量的前37%的样本**做为==参考==，其中的**最优的那个**作为==对照点==，**如果在剩下63%的样本中，出现比对照点好的样本，就果断选择它。**

#### 2. 某大公司有这么一个规定：只要有一个员工过生日，当天所有员工全部放假一天。但在其余时候，所有员工都没有假期，必须正常上班。这个公司需要雇用多少员工，才能让公司一年内所有员工的总工作时间期望值最大？

> 假设一年有365 天，每个员工的生日都概率均等地分布在这 365 天里。
>
> $E = n*(1-1/365)^n$
>
> 解：E有最大值，则
>
> $n*(1-1/365)^n \geq (n+1) (1-1/365)^{n+1}$
>
> 且
>
> $n*(1-1/365)^n \geq (n-1) (1-1/365)^{n-1}$
>
> 解得$364 \leq n \leq 365$
>
> n=365

- 参考链接：http://www.tilaile.com/question/14599

#### 3. 切比雪夫不等式

> $P(|X-\mu| \geq k\sigma) \leq \frac{1}{k^2}，k > 0，\mu为期望，\sigma为标准差$

#### 4. 一根绳子，随机截成3段，可以组成一个三角形的概率有多大

> 设绳子长为a，折成三段的长度为x，y，a-x-y。从而得到x>0，y>0，a-x-y>0，满足这三个约束条件在平面直角坐标系中的可行域为一个直角三角形，面积为$\frac{1}{2} a^2$。而构成三角形的条件，任意两边的和大于第三边的条件$x+y>a-x-y， a-y>y， a-x>x$同时成立。
>
> 满足以上不等式在平面直角坐标系中也是一个直角三角形，面积为$\frac{1}{8} a^2$，所以构成三角形概率为$\frac{\frac{1}{2} a^2}{\frac{1}{8} a^2}=0.25$

#### 5. 最大似然估计（MLE）和最大后验概率（MAP）的区别?

> 最大似然估计提供了一种给定观察数据来评估模型参数的方法，而最大似然估计中的采样满足所有采样都是独立同分布的假设。最大后验概率是根据经验数据获难以观察量的点估计，与最大似然估计最大的不同是**最大后验概率融入了要估计量的先验分布在其中**，所以最大后验概率可以看做规则化的最大似然估计。

- 参考链接：[最大似然估计与最大后验概率的区别与联系](https://blog.csdn.net/laobai1015/article/details/78062767)

#### 6. 什么是共轭先验分布

> 假设$\theta$为总体分布中的参数，$\theta$的先验密度为$\pi(\theta)$，而抽样信息算得的后验密度函数与$\pi(\theta)$具有相同的函数形式，则称$\pi(\theta)$为$\theta$的共轭先验分布。

- 参考链接：[共轭先验分布](https://blog.csdn.net/u010945683/article/details/49149815)

#### 7. 概率（likelihood）和似然（probability）的区别

> 概率是指在给定参数$\theta$的情况下，样本的随机向量X=x的可能性。
>
> 而似然表示的是在给定样本X=x的情况下，参数$\theta$为真实值的可能性。
>
> 一般情况，对随机变量的取值用概率表示。而在非贝叶斯统计的情况下，参数为一个实数而不是随机变量，一般用似然来表示。

- 参考链接：[似然（likelihood）和概率（probability）的区别与联系](https://blog.csdn.net/songyu0120/article/details/85059149)

#### 8. 频率学派和贝叶斯学派的区别

> 频率派认为抽样是无限的，在无限的抽样中，对于决策的规则可以很精确。贝叶斯派认为世界无时无刻不在改变，未知的变量和事件都有一定的概率，即后验概率是先验概率的修正。频率派认为模型参数是固定的，一个模型在无数次抽样后，参数是不变的。而贝叶斯学派认为数据才是固定的而参数并不是。频率派认为模型不存在先验而贝叶斯派认为模型存在先验。

#### 9. 0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器

> 均匀分布均值为1/2，方差为1/12。要变为均值为0，方差为1则xv变换为√12(x–1/2)。

- 参考链接：[概率论中均匀分布的数学期望和方差](https://zhidao.baidu.com/question/757846050189625524.html)

#### 10. Lasso的损失函数

> $J(\theta) = \frac{1}{2} ||y-Xw||^2 + \lambda \sum |\theta|$

- Ridge回归/岭回归：它和一般线性回归的区别是在损失函数上增加了一个L2正则化的项。
- Lasso回归：它和一般线性回归的区别是在损失函数上增加了一个L1正则化的项。

- 参考链接1：[Lasso回归算法： 坐标轴下降法与最小角回归法小结](https://www.cnblogs.com/pinard/p/6018889.html)
- 参考链接2：[岭回归、Lasso及其分析](https://blog.csdn.net/Byron309/article/details/77716127)

#### 11. Sfit特征提取和匹配的具体步骤

> 生成高斯差分金字塔，尺度空间构建，空间极值点检测，稳定关键点的精确定位，稳定关键点方向信息分配，关键点描述，特征点匹配。

- 参考链接：[Sift算子特征点提取、描述及匹配全流程解析](https://blog.csdn.net/dcrmg/article/details/52577555)

### 线性代数

#### 1. 求$m*k$矩阵A和$n*k$矩阵的欧几里得距离?

