# 第8章 降维

## 写在前面

**参考书**

《机器学习实战——基于Scikit-Learn和TensorFlow》

**工具**

python3.5.1，Jupyter Notebook, Pycharm

## 数据降维的两种主要方法

- 投影
- 流形学习
  - 流形假设：也称为流形假说，认为大多数现实世界的高维度数据集存在一个低维度的流形来重新表示。这个假设通常是凭经验观察的。
  - 流形假设通常还伴随着一个隐含的假设：如果能用低维空间的流形表示，手头的任务（例如分类或者回归）将变得更简单。

## PCA

- 主成分分析（PCA）是迄今为止最流行的降维算法。它先是识别出最接近数据的超平面，然后将数据投影其上。

- 比较原始数据集与其轴上的投影之间的均方距离，使这个均方距离最小的轴使最合理的选择。这也正是PCA背后的简单思想。

- 奇异值分解（SVD）

  ```python
  import numpy as np
  
  X = np.random.random((50, 2))
  X_centered = X - X.mean(axis=0)
  U, s, V = np.linalg.svd(X_centered)
  c1 = V.T[:, 0]
  c2 = V.T[:, 1]
  
  W2 = V.T[:, :1]
  W2D = X_centered.dot(W2)
  ```

- **PCA实战**

  ```python
  from sklearn.decomposition import PCA
  
  pca = PCA(n_components=1)
  X2D = pca.fit_transform(X)
  ```

  - components_：访问主成分（它包含的主成分使水平向量）。举例来说，第一个主成分即等于

    ```python
    pca.components_.T[:, 0]
    ```

  - explained_variance_ratio_：方差解释率。它表示每个主成分轴对整个数据集方差的贡献度。

  - inverse_transform()：将压缩后的数据解压缩回到原来的shape。原始数据和重建数据（压缩之后解压缩）之间的均方距离成为**重建误差**。

- 选择正确数量的维度：

  - 设置n_components=d，若d是整数，则设置为降维后的维度数；若d是小数，则表示希望保留的方差比。
  - 还可以将解释方差绘制关于维度数量的函数（绘制np.cumsum即可）。曲线通常都会有一个拐点，说明**方差停止快速增长**。你可以将其视为数据集的本征维数。

- **增量PCA（IPCA）**

  - 增量主成分分析
  
- 可以将训练集分成一个个小批量，一次给IPCA
  
      ```python
      import numpy as np
      from sklearn.decomposition import IncrementalPCA
    from sklearn.datasets import fetch_mldata
  
      n_batches = 100
      mnist = fetch_mldata('MNIST original')
    X_mnist = mnist["data"]
  
      inc_pca = IncrementalPCA(n_components=154)
      for X_batch in np.array_split(X_mnist, n_batches):
          inc_pca.partial_fit(X_batch)
	    X_mnist_reduced = inc_pca.transform(X_mnist)
	```
  
- 还可以使用Numpy的memmap类，它允许你巧妙地操控一个存储在磁盘二进制文件的大型数组，就好似它也完全在内存里一样，而这个类（memmap）仅在需要时加载内存中需要的数据。
  
  - 内存映射，当需要存取一个很大的文件里面的小部分的数据的时候，读入整个文件显然是非常的浪费资源的。于是要使用到内存映射的方法。
  
  - [numpy中的numpy.memmap函数的用法](https://blog.csdn.net/KID_yuan/article/details/89019384)
  
    ```python
  X_mm = np.memmap(filename, dtype="float32", mode="readonly", shape=(m, n))
    batch_size = m // n_batches
    inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)
    inc_pca.fit(X_mm)
    ```
  
- **随机PCA**

  - 这是一个随机算法，可以快速找到前d个主成分的近似值。
  - 它的计算复杂度是$O(m\times d^2) + O(d^3)$，而不是$O(m \times n^2) + O(n^3)$。
  - 所以当d远小于n时，它比前面提到的算法要快得多。
  - 使用时，设置PCA中的参数svd_solver=“randomized”

## 核主成分分析

------

我的CSDN：https://blog.csdn.net/qq_21579045

我的博客园：https://www.cnblogs.com/lyjun/

我的Github：https://github.com/TinyHandsome

纸上得来终觉浅，绝知此事要躬行~

欢迎大家过来OB~

by 李英俊小朋友