# 第10章_人工神经网络简介

[TOC]

**参考书**

《机器学习实战——基于Scikit-Learn和TensorFlow》

**工具**

python3.5.1，Jupyter Notebook, Pycharm

## 感知器

- 《行为的组织》，1949，Donald Hebb：如果一个生物神经元总是出发另外的神经元，那么这两个神经元之间的连接就会变得更强。
- Siegrid Lowel：同时处于激活状态的细胞时会连在一起的。
- Hebb定律（又叫Hebbian学习）：当两个神经元有相同的输出时，它们之间的连接权重就会增强。
- 感知器收敛定理（Rosenblatt）：如果训练实例是线性可分的，这个算法会收敛到一个解。

## 线性阈值单元（LTU）

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import Perceptron

iris = load_iris()
X = iris.data[:, (2, 3)]
y = (iris.target == 0).astype(np.int)
per_clf = Perceptron(random_state=42)
per_clf.fit(X, y)

y_pred = per_clf.predict([[2, 0.5]])
print(y_pred)
```

- 事实上，在sklearn中，Perceptron类的行为等同于使用以下超参数的**SGDClassifier**：

  ```python
  loss="perceptron", learning_rate="constant", eta0=1（学习速率）, penalty=None（不做正则化）
  ```

- 注意和逻辑回归分类器相反，感知器不输出某个类的概率，它只能根据一个固定的阈值来做预测。这也是更应该使用逻辑回归而不是感知器的一个原因。

- 感知器无法处理一些很微小的问题，比如异或分类问题（XOR），不过事实证明感知器的一些限制可以通过多层感知器来解决（Multi-Layer Perceptron，MLP）

## 多层感知器和反向传播

- 如果一个ANN有2个及以上的隐藏层，则被称为深度神经网络（DNN）。
- 反向自动微分的梯度下降法：对于每个训练实例，反向传播算法先做一次预测（**正向过程**），度量误差，然后反向的遍历每个层次来度量每个连接的误差贡献度（**反向过程**），最后再微调每个连接的权重来降低误差（**梯度下降**）。

## 使用TensorFlow的高级API来训练MLP

- 用DNNClassifier类来训练一个有着任意数量隐藏层，并包含一个用来计算类别概率的sofmax输出层的神经网络。

  ```python
  from tensorflow.examples.tutorials.mnist import input_data
  import tensorflow as tf
  
  mnist = input_data.read_data_sets("D:/李添的数据哦！！！/BookStudy/book4/MNIST_data/MNIST_data/")
  
  X_train = mnist.train.images
  X_test = mnist.test.images
  y_train = mnist.train.labels.astype("int")
  y_test = mnist.test.labels.astype("int")
  
  config = tf.contrib.learn.RunConfig(tf_random_seed=42) # not shown in the config
  
  feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)
  dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300,100], n_classes=10,
                                           feature_columns=feature_cols, config=config)
  dnn_clf = tf.contrib.learn.SKCompat(dnn_clf) # if TensorFlow >= 1.1
  dnn_clf.fit(X_train, y_train, batch_size=50, steps=40000)
  ```
  
- 模型评估

  ```python
  from sklearn.metrics import accuracy_score
  
  y_pred = dnn_clf.predict(X_test)
  accuracy_score(y_test, y_pred['classes'])
  ```

  ```python
  from sklearn.metrics import log_loss
  
  y_pred_proba = y_pred['probabilities']
  log_loss(y_test, y_pred_proba)
  ```

- 库还包含了一些方便的函数来评估模型：

  ```python
  dnn_clf.score(X_test, y_test)
  ```

## 使用纯TensorFlow训练DNN

### 1. 构建阶段

- 创建用于输入和目标值占位符节点

  ```python
  import tensorflow as tf
  import numpy as np
  
  n_inputs = 28*28
  n_hidden1 = 300
  n_hidden2 = 100
  n_outputs = 10
  
  X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")
  y = tf.placeholder(tf.int64, shape=(None), name="y")
  ```

- 创建用以创建神经网络的函数，使用它创建DNN

  - 手动

    ```python
    def neuron_layer(X, n_neurons, name, activation=None):
        with tf.name_scope(name):
            n_inputs = int(X.get_shape()[1])
            stddev = 2 / np.sqrt(n_inputs)
            init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)
            W = tf.Variable(init, name="weights")
            b = tf.Variable(tf.zeros([n_neurons]), name="biases")
            z = tf.matmul(X, W) + b
            if activation == "relu":
                return tf.nn.relu(z)
            else:
                return z
    
    with tf.name_scope("dnn"):
        hidden1 = neuron_layer(X, n_hidden1, "hidden1", activation="relu")
        hidden2 = neuron_layer(hidden1, n_hidden2, "hidden2", activation="relu")
        logits = neuron_layer(hidden2, n_outputs, "outputs")
    ```

  - 利用fully_connected()函数来替换自己写的neuron_layer()函数

    ```python
    from tensorflow.contrib.layers import fully_connected
    
    with tf.name_scope("dnn"):
        hidden1 = fully_connected(X, n_hidden1, scope="hidden_1")
        hidden2 = fully_connected(hidden1, n_hidden2, scope="hidden_2")
        logits2 = fully_connected(hidden2, n_outputs, scope="outputs_", activation_fn=None)
    ```

- 定义成本函数

  ```python
  with tf.name_scope("loss"):
      xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits2)
      loss = tf.reduce_mean(xentropy, name="loss")
  ```

- 创建优化器

  ```python
  learning_rate = 0.01
  with tf.name_scope("train"):
      optimizer = tf.train.GradientDescentOptimizer(learning_rate)
      training_op = optimizer.minimize(loss)
  ```

- 定义性能度量

  ```python
  with tf.name_scope("eval"):
      correct = tf.nn.in_top_k(logits2, y, 1)
      accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))
  ```

- 最后初始化和保存模型

  ```python
  init = tf.global_variables_initializer()
  saver = tf.train.Saver()
  ```

- `tf.truncated_normal((n_inputs, n_neurons), stddev=np.sqrt(n_inputs))`：使用标准偏差为2/sqrt(n_inputs)的阶段正态（高斯）分布进行随机初始化。<u>使用截断的正态分布而不是常规的正态分布，保证这里不存在任何减慢训练的大权重。</u>使用一个指定的标准偏差会让算法收敛得更快。
- 为所有隐藏层随机地初始化连接权重值是非常重要的，这可以避免任何可能导致梯度下降出现无法终止的对称性。
- `tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)`：它会根据“logits”来计算交叉熵（比如，在通过softmax激活函数之前网络的输出），并且期望以0到分类个数减1的整数形式标记。这会计算出一个包含每个实例的交叉熵的一维张量。可以用TensorFlow的reduce_mean()函数来计算所有实例的平均交叉熵。
- 函数`sparse_softmax_cross_entropy_with_logits()`与先应用softmax函数再计算交叉熵的效果是一样的，不过它更高效一些，另外它会处理一些边界值如logits等于0的情况。这也是为什么我们之前没有使用softmax激活函数的原因。此外，**还有一个`softmax_cross_entropy_with_logits()`函数，它以one-hot的形式获取标签（而不是从0到分类数量-1）**

### 2. 执行阶段



------

我的CSDN：https://blog.csdn.net/qq_21579045

我的博客园：https://www.cnblogs.com/lyjun/

我的Github：https://github.com/TinyHandsome

纸上得来终觉浅，绝知此事要躬行~

欢迎大家过来OB~

by 李英俊小朋友