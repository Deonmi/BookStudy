# 第10章_人工神经网络简介

[TOC]

**参考书**

《机器学习实战——基于Scikit-Learn和TensorFlow》

**工具**

python3.5.1，Jupyter Notebook, Pycharm

## 感知器

- 《行为的组织》，1949，Donald Hebb：如果一个生物神经元总是出发另外的神经元，那么这两个神经元之间的连接就会变得更强。
- Siegrid Lowel：同时处于激活状态的细胞时会连在一起的。
- Hebb定律（又叫Hebbian学习）：当两个神经元有相同的输出时，它们之间的连接权重就会增强。
- 感知器收敛定理（Rosenblatt）：如果训练实例是线性可分的，这个算法会收敛到一个解。

## 线性阈值单元（LTU）

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import Perceptron

iris = load_iris()
X = iris.data[:, (2, 3)]
y = (iris.target == 0).astype(np.int)
per_clf = Perceptron(random_state=42)
per_clf.fit(X, y)

y_pred = per_clf.predict([[2, 0.5]])
print(y_pred)
```

- 事实上，在sklearn中，Perceptron类的行为等同于使用以下超参数的**SGDClassifier**：

  ```python
  loss="perceptron", learning_rate="constant", eta0=1（学习速率）, penalty=None（不做正则化）
  ```

- 注意和逻辑回归分类器相反，感知器不输出某个类的概率，它只能根据一个固定的阈值来做预测。这也是更应该使用逻辑回归而不是感知器的一个原因。

- 感知器无法处理一些很微小的问题，比如异或分类问题（XOR），不过事实证明感知器的一些限制可以通过多层感知器来解决（Multi-Layer Perceptron，MLP）

## 多层感知器和反向传播

- 如果一个ANN有2个及以上的隐藏层，则被称为深度神经网络（DNN）。
- 反向自动微分的梯度下降法：对于每个训练实例，反向传播算法先做一次预测（**正向过程**），度量误差，然后反向的遍历每个层次来度量每个连接的误差贡献度（**反向过程**），最后再微调每个连接的权重来降低误差（**梯度下降**）。

## 使用TensorFlow的高级API来训练MLP



------

我的CSDN：https://blog.csdn.net/qq_21579045

我的博客园：https://www.cnblogs.com/lyjun/

我的Github：https://github.com/TinyHandsome

纸上得来终觉浅，绝知此事要躬行~

欢迎大家过来OB~

by 李英俊小朋友