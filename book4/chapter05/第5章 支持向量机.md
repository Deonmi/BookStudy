# 第5章 支持向量机

## 写在前面

**参考书**

《机器学习实战——基于Scikit-Learn和TensorFlow》

**工具**

python3.5.1，Jupyter Notebook, Pycharm

## 核函数

- linear、poly、rbf、sigmoid。
- 有那么多核函数，该如何决定使用哪一个呢？<u>有一个经验法则是：永远先从线性核函数开始尝试。</u>
- 要记住，LinearSVC比SVC(kernel=“linear”)快得多，特别是训练集非常大或者特征非常多的时候。
- 如果训练集不大，可以试试高斯RBF核，大多数情况下它都非常好用。

## 用于SVM分类的sklearn类的比较

| 类            | 时间复杂度            | 是否支持核外 | 是否需要缩放 | 核技巧 |
| ------------- | --------------------- | ------------ | ------------ | ------ |
| LinearSVC     | O(mxn)                | 否           | 是           | 否     |
| SGDClassifier | O(mxn)                | 是           | 是           | 否     |
| SVC           | O($m^2$xn)-O($m^3$xn) | 否           | 是           | 是     |

## 练习摘抄

- 使用SVM时，对输入值进行缩放为什么重要？

  支持向量机拟合类别之间可能的/最宽的街道，所以如果训练集不经过缩放，SVM将趋于忽略值较小的特征。

- SVM分类器在对实例进行分类时，会输出信心分数吗？概率呢？

  支持向量机分类器能够输出测试实例与决策边界之间的距离，你可以将其作为信心分数。但是这个分数不能直接转化为类别概率的估算。如果创建SVM时，在sklearn中设置probability=True，那么训练完后，算法将使用逻辑回归对SVM分数进行校准（对训练数据额外进行5折交叉验证的训练），从而得到概率值。这会给SVM添加predict_proba()核predict_log_proba()两种方法。

- 如果训练集有上千万个实例核几百个特征，你应该使用SVM原始问题还是对偶问题来训练模型呢？

  这个问题仅适用于线性支持向量机，因为核SVM只能使用对偶问题。对于SVM问题来说，原始形式的计算复杂度与训练实例的数量成正比，而其对偶形式的计算复杂度与某个介于$m^2$和$m^3$之间的数量成正比。所以如果实例的数量以百万计，一定要使用原始问题，因为对偶问题会非常慢。

------

我的CSDN：https://blog.csdn.net/qq_21579045

我的博客园：https://www.cnblogs.com/lyjun/

我的Github：https://github.com/TinyHandsome

纸上得来终觉浅，绝知此事要躬行~

欢迎大家过来OB~

by 李英俊小朋友