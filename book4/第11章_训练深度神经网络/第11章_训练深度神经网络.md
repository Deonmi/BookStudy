# 第11章_训练深度神经网络

[TOC]

**参考书**

《机器学习实战——基于Scikit-Learn和TensorFlow》

**工具**

python3.5.1，Jupyter Notebook, Pycharm

## 梯度消失/爆炸问题

- **梯度消失**：梯度经常会随着算法进展到更低层时变得越来越小。导致的结果是，梯度下降在更低层网络连接权值更新方面基本没有改变，而且训练不会收敛到好的结果。
- **梯度爆炸**：在一些例子中会发生相反的现象，梯度越来越大，导致很多层的权值疯狂增大，使得算法发散。（经常出现在循环神经网络中）。
- 简单来讲，深度神经网络受制于不稳定梯度，不同层可能会以完全不同的速度学习。

## Xavier初始化和He初始化

- 在Glorot和Bengio的论文中提出，当预测的时候要保持正向，在反向传播梯度的时保持反方向。为了让信号正确流动，作者提出需要保持每一层的输入和输出的方差一致，并且需要在反向流动过某一层时，前后的方差也要一致。<u>事实上，这是很难保证的，除非一层有相同数量的输入和输出连接。因此它们提出了一个很好的这种方案：</u>

- 连接的权重必须按照以下公式进行随机初始化，其中$n_{inputs}$和$n_{outputs}$是权重被初始化层的输入和输出连接数（也称为扇入和扇出）。这种初始化方法称为**Xavier初始化**（当使用**逻辑激活函数**时）。同理，**ReLU激活函数**的初始化方法及其变种称为**He初始化**。

  | 激活函数       | 均匀分布-r、r                                           | 正态分布                                                     |
  | -------------- | ------------------------------------------------------- | ------------------------------------------------------------ |
  | 逻辑函数       | $r=\sqrt{ \frac{6}{n_{inputs} + n_{outputs}} }$         | $\sigma=\sqrt{ \frac{2}{n_{inputs} + n_{outputs}} }$         |
  | 双曲正切函数   | $r=\sqrt[4]{ \frac{6}{n_{inputs} + n_{outputs}} }$      | $\sigma=\sqrt[4]{ \frac{2}{n_{inputs} + n_{outputs}} }$      |
  | ReLU（及变种） | $r=\sqrt[\sqrt2]{ \frac{6}{n_{inputs} + n_{outputs}} }$ | $\sigma=\sqrt[\sqrt2]{ \frac{2}{n_{inputs} + n_{outputs}} }$ |

## 非饱和激活函数

- ReLU函数并不是完美的。它会出现**dying ReLU问题**：在训练过程中，一些神经元实际上已经死了，即它们只输出0。在训练过程中，如果神经元的权重更新到神经元输入的总权重是负值时，这个神经元就会开始输出0。当这种情况发生时，除非ReLU函数的梯度为0并且输入为负，否则这个神经元就不会再重新开始工作。

- 要解决这个问题，可以使用ReLU函数的变种，比如**leaky ReLU（带泄露线性整流函数）**。这个函数定义为$LeakyReLU_a(z)=max(az, z)$。超参数a表示函数“泄露”程度：它是函数中z<0时的坡度，这个小坡度可以保证leaky ReLU不会死，它可以进入一个很长的昏迷其，但最后还是有机会醒过来。

- 最后，Clevert等人提出了一个新的激活函数，称为ELU（加速线性单元），它的表现优于ReLU的所有变种：训练时间减小，神经网络再测试集的表现也更好。

  $$ELU_a(z)=
  \begin{cases}
  a(exp(z)-1) & z<0 \\
  z & z \geq 0
  \end{cases}$$
  
  - 当$z \leq 0$时它的值为负，从而允许单元的平均输出接近0。这样就可以缓和梯度消失的问题。超参数a是指当z是一个极大的负数时，ELU函数接近的那个值。
  - 对于z<0有一个非零的梯度，这样就可以避免单元消失的问题。
  - 这个函数整体很平滑，包括在z=0附近，这样就可以提高梯度下降，因为在z=0的左右两边都没有抖动。
  - ELU激活函数的一个主要缺陷是计算速度比ReLU和它的变种慢（因为使用了指数函数），但是在训练过程中，可以通过更快的收敛速度来弥补。然而，测试中，ELU网络时间慢于ReLU网络。
  
- **那么在你的深度神经网络的隐藏层到底应该使用哪一种激活函数呢？**

  - 通常来说ELU函数>leaky ReLU函数（和它的变种）>ReLU函数>tanh函数>逻辑函数。
  - 如果你更关心运行时的性能，那你可以选择leaky ReLU函数，而不是ELU函数。
  - 如果你不想改变别的超参数，就只使用建议a的默认值（leaky ReLU函数是0.01，ELU函数是1）。
  - 如果你有多余的时间和计算能力，可以使用交叉验证去评估别的激活函数
  - 如果你的网络过拟合，可以使用RReLU函数
  - 如果针对大的训练集，可以使用PReLU函数。

- 使用elu激活函数：`hidden1 = fully_connected(X, n_hidden1, activation_fn=tf.nn.elu)`

- 使用leaky ReLU激活函数：

  ```python
  def leaky_relu(z, name=None):
  	return tf.maximum(0.01 * z, z, name=name)
  hidden1 = fully_connected(X, n_hidden1, activation_fn=leaky_relu)
  ```

## 批量归一化

- 尽管使用了He初始化加ELU可以很明显地在训练初期降低梯度消失/爆炸问题，但还是不能保证在训练过程中不会再出现这些问题。
- 用来解决梯度消失/爆炸的问题，而且每一层的输入分散问题再训练过程中更普遍，前层变量的改变（称为内部协变量转变问题）也是一样。
- 该技术包括在每一层激活函数之前在模型里加一个操作，**简单零中心化**和**归一化输入**，之后再通过每层的两个新参数（一个为了缩放，一个为了移动）缩放和移动结果。换句话说，**这个操作让模型学会了最佳规模和每层输入的平均值。**
- 整体看来，4个参数是为每一批量归一化层来学习的：$\gamma$（缩放），$\beta$（偏移），$\mu$（平均值）和$\sigma$（标准方差）。
- 批量归一化同时还可以进行正则化，降低其他正则化技术的需求。
- 你会发现一开始当梯度下降再每一层搜索最佳缩放和偏移量时，训练速度会非常慢，但是一旦找到一个合适的值，训练速度就会迅速提升。

### 用TensorFlow来实现批量归一化



------

我的CSDN：https://blog.csdn.net/qq_21579045

我的博客园：https://www.cnblogs.com/lyjun/

我的Github：https://github.com/TinyHandsome

纸上得来终觉浅，绝知此事要躬行~

欢迎大家过来OB~

by 李英俊小朋友