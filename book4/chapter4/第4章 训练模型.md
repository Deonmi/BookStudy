# 第4章 训练模型

## 写在前面

**参考书**

《机器学习实战——基于Scikit-Learn和TensorFlow》

**工具**

python3.5.1，Jupyter Notebook, Pycharm

## 线性回归算法比较

| 算法           | m很大 | 是否支持核外 | n很大 | 超参数 | 是否需要缩放 | sklearn          |
| -------------- | ----- | ------------ | ----- | ------ | ------------ | ---------------- |
| 标准方程       | 快    | 否           | 慢    | 0      | 否           | LinearRegression |
| 批量梯度下降   | 慢    | 否           | 快    | 2      | 是           | n/a              |
| 随机梯度下降   | 快    | 是           | 快    | ≥2     | 是           | SGDRegressor     |
| 小批量梯度下降 | 快    | 是           | 快    | ≥2     | 是           | n/a              |

其中，m是训练实例的数量，n是特征数量。

## 多项式特征

- from sklearn.preprocessing import PolynomialFeatures
- PolynomialFeatures会在给定的多项式阶数下，添加所有的特征组合
- 例如两个特征a，b，阶数degree=3，PolynomialFeatures不只会添加特征$a^2$、$a^3$、$b^2$、$b^3$，还会添加组合$ab$、$a^2b$、$ab^2$。

## 偏差/方差权衡

- 在统计学和机器学习领域，一个重要的理论结果是，模型的泛化误差可以被表示为三个截然不同的误差之和：
  - 偏差：错误的假设（假设数据是线性的）。
  - 方差：对数据的微小变化过度敏感（过拟合）。
  - 不可避免的误差：噪声（通过清理数据来改善噪声）。

## 线性回归

- 普通线性回归：from sklearn.linear_model import LinearRegression
- 岭回归（l2范数正则化）：from sklearn.linear_model import Ridge
- 套索回归（l1范数正则化）：from sklearn.linear_model import Lasso
- 弹性网络（岭回归和套索回归的混合）：from sklearn.linear_model import ElasticNet

## 练习部分摘抄

- 如果你的训练集里特征的数值大小迥异，什么算法可能会受到影响？受影响程度如何？你应该怎么做？

  如果训练集的特征数值具有非常迥异的尺寸比例，成本函数将呈现为细长的碗状，这导致梯度下降算法将耗费很长时间来收敛。要解决这个问题，需要在训练模型之前先对数据进行缩放。值得注意的是，使用标准方程法，不经过特征缩放也能正常工作。

- 训练逻辑回归模型时，梯度下降是否会困于局部最小值？

  不会，因为它的成本函数是凸函数。

- 假设你使用的是批量梯度下降，并且每一轮训练都绘制出其验证误差，如果发现验证误差持续上升，可能发生了什么？你如何解决这个问题？

  可能性之一是学习率太高，算法开始发散所致。如果训练误差也开始上升，那么很明显你要降低学习率了。但是，如果训练误差没有上升，那么模型可能过度拟合训练集，应该立即停止训练。

- 哪种梯度下降算法能最快达到最优解的附近？那种会收敛？如何使其他算法同样收敛？

  随机梯度下降的训练迭代最快，因为它一次只考虑一个训练实例，所以通常来说，它会最快到达全局最优的附近（或者是批量非常小的小批量梯度下降）。但是，只有批量梯度下降才会经过足够长时间的训练后真正收敛。对于随机梯度下降和小批量梯度下降来说，除非逐渐调低学习率，否则将一直围绕最小值上上下下。

------

我的CSDN：https://blog.csdn.net/qq_21579045

我的博客园：https://www.cnblogs.com/lyjun/

我的Github：https://github.com/TinyHandsome

纸上得来终觉浅，绝知此事要躬行~

欢迎大家过来OB~

by 李英俊小朋友