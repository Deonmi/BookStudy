# 第6章 决策树

## 写在前面

**参考书**

《机器学习实战——基于Scikit-Learn和TensorFlow》

**工具**

python3.5.1，Jupyter Notebook, Pycharm

## 投票分类器

- 使用不同的训练方法训练同样的数据集。

- from sklearn.ensemble import VotingClassifier：

  ```python
  voting_clf = VotingClassifier(
      estimators=[
          ('lr', log_clf),
          ('rf', rnd_clf),
          ('svc', svm_clf)
      ],
      voting='hard'
  )
  voting_clf.fit(X_train, y_train)
  ```

## bagging和pasting

- 每个预测器使用的算法相同，但是在不同的训练集随机子集上进行训练。

- 采样时样本放回：bagging(boostrap aggregating，自举汇聚法)，统计学中，放回重新采样称为自助法(bootstrapping)。

- 采样时样本不放回：pasting。

- from sklearn.ensemble import BaggingClassifier

  ```python
  bag_clf = BaggingClassifier(
      DecisionTreeClassifier(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1
  )
  bag_clf.fit(X_train, y_train)
  ```

- 如果想使用pasting，只需要设置bootstrap=False

- 集成预测的泛化效果很可能比单独的分类器要好一些：二者偏差相近，但是集成的方差更小。（两边训练集上的错误数量差不多，但是集成的决策边界更规则）

- 由于自助法给每个预测器的训练子集引入了更高的多样性，所以最后bagging比pasting的偏差略高，但这也意味着预测器之间的关联度更低，所以集成的方差降低。

- **总之，bagging生成的模型通常更好。**

## 包外评估

- 对于任意给定的预测器，使用bagging，有些实例可能会被采样多次，而有些实例则可能根本不被采样。

- BaggingClassifier默认采样m个训练实例，然后放回样本(bootstrap=True)，m是训练集的大小。这意味着对于每个预测器来说，平均只对63%的训练实例进行采样。（随着m增长，这个比率接近$1-exp(-1)\approx63.212%$）。剩余37%未被采样的训练实例成为**包外(oob)**实例。注意，对所有预测其来说，这是不一样的37%。

- 同个设置oob_score=True自动进行包外评估，通过变量oob_score_可以得到最终的评估分数。

  ```python
  bag_clf = BaggingClassifier(
      DecisionTreeClassifier(), n_estimators=500, bootstrap=True, n_jobs=-1, oob_score=True
  )
  bag_clf.fit(X_train, y_train)
  bag_clf.oob_score_
  ```

- 每个训练实例的包外决策函数也可以通过变量oob_decision_function_获得。

  ```
  bag_clf.oob_decision_function_
  ```

## Random Patches和随机子空间

- BaggingClassifier也支持对特征进行抽样，这通过两个超参数控制：`max_features`和`bootstrap_features`。它们的工作方式跟max_samples和bootstrap相同，只是抽样对象不再是实例，而是特征。因此，每个预测器将用输入特征的随机子集进行训练。
- 这对于处理高维输入特别有用（例如图像）。对训练实例和特征都进行抽样，被称为Random Patches方法。而保留所有训练实例（即bootstrap=False并且max_samples=1.0）但是对特征进行抽样（即bootstrap_features=True并且/或max_features<1.0），这被称为随机子空间法。

## 极端随机树

- 随机森林里单棵树的生长过程中，每个节点在分裂时仅考虑到了一个随机子集所包含的特征。如果我们对每个特征使用随机阈值，而不是搜索得出的最佳阈值（如常规决策树），则可能让决策树生长得更加随机。
- 这种极端随机的角儿参数组成的森林，被称为极端随机树（Extra-Trees）。
- 它也是以更高的偏差换取了更低的方差。
- 极端随机树训练起来比常规随机森林要快很多，因为在每个节点上找到每个特征的最佳阈值是决策树生长中最耗时的任务之一。
- from sklearn.ensemble import ExtraTreesClassifier

------

我的CSDN：https://blog.csdn.net/qq_21579045

我的博客园：https://www.cnblogs.com/lyjun/

我的Github：https://github.com/TinyHandsome

纸上得来终觉浅，绝知此事要躬行~

欢迎大家过来OB~

by 李英俊小朋友