# 第6章 决策树

## 写在前面

**参考书**

《机器学习实战——基于Scikit-Learn和TensorFlow》

**工具**

python3.5.1，Jupyter Notebook, Pycharm

## 投票分类器

- 使用不同的训练方法训练同样的数据集。

- from sklearn.ensemble import VotingClassifier：

  ```python
  voting_clf = VotingClassifier(
      estimators=[
          ('lr', log_clf),
          ('rf', rnd_clf),
          ('svc', svm_clf)
      ],
      voting='hard'
  )
  voting_clf.fit(X_train, y_train)
  ```

## bagging和pasting

- 每个预测器使用的算法相同，但是在不同的训练集随机子集上进行训练。

- 采样时样本放回：bagging(boostrap aggregating，自举汇聚法)，统计学中，放回重新采样称为自助法(bootstrapping)。

- 采样时样本不放回：pasting。

- from sklearn.ensemble import BaggingClassifier

  ```python
  bag_clf = BaggingClassifier(
      DecisionTreeClassifier(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1
  )
  bag_clf.fit(X_train, y_train)
  ```

- 如果想使用pasting，只需要设置bootstrap=False

- 集成预测的泛化效果很可能比单独的分类器要好一些：二者偏差相近，但是集成的方差更小。（两边训练集上的错误数量差不多，但是集成的决策边界更规则）

- 由于自助法给每个预测器的训练子集引入了更高的多样性，所以最后bagging比pasting的偏差略高，但这也意味着预测器之间的关联度更低，所以集成的方差降低。

- **总之，bagging生成的模型通常更好。**

## 包外评估

- 对于任意给定的预测器，使用bagging，有些实例可能会被采样多次，而有些实例则可能根本不被采样。

- BaggingClassifier默认采样m个训练实例，然后放回样本(bootstrap=True)，m是训练集的大小。这意味着对于每个预测器来说，平均只对63%的训练实例进行采样。（随着m增长，这个比率接近$1-exp(-1)\approx63.212%$）。剩余37%未被采样的训练实例成为**包外(oob)**实例。注意，对所有预测其来说，这是不一样的37%。

- 同个设置oob_score=True自动进行包外评估，通过变量oob_score_可以得到最终的评估分数。

  ```python
  bag_clf = BaggingClassifier(
      DecisionTreeClassifier(), n_estimators=500, bootstrap=True, n_jobs=-1, oob_score=True
  )
  bag_clf.fit(X_train, y_train)
  bag_clf.oob_score_
  ```

- 每个训练实例的包外决策函数也可以通过变量oob_decision_function_获得。

  ```
  bag_clf.oob_decision_function_
  ```

## Random Patches和随机子空间

- BaggingClassifier也支持对特征进行抽样，这通过两个超参数控制：`max_features`和`bootstrap_features`。它们的工作方式跟max_samples和bootstrap相同，只是抽样对象不再是实例，而是特征。因此，每个预测器将用输入特征的随机子集进行训练。
- 这对于处理高维输入特别有用（例如图像）。对训练实例和特征都进行抽样，被称为Random Patches方法。而保留所有训练实例（即bootstrap=False并且max_samples=1.0）但是对特征进行抽样（即bootstrap_features=True并且/或max_features<1.0），这被称为随机子空间法。

## 极端随机树

- 随机森林里单棵树的生长过程中，每个节点在分裂时仅考虑到了一个随机子集所包含的特征。如果我们对每个特征使用随机阈值，而不是搜索得出的最佳阈值（如常规决策树），则可能让决策树生长得更加随机。
- 这种极端随机的角儿参数组成的森林，被称为极端随机树（Extra-Trees）。
- 它也是以更高的偏差换取了更低的方差。
- 极端随机树训练起来比常规随机森林要快很多，因为在每个节点上找到每个特征的最佳阈值是决策树生长中最耗时的任务之一。
- from sklearn.ensemble import ExtraTreesClassifier

##  特征重要性

- 如果你查看单个决策树会发现，重要的特征更可能出现靠近根节点的位置，而不重要的特征通常出现在靠近叶节点的位置（甚至根本不出现）。
- 因此可以通过计算一个特征在森林中所有树上的平均深度，可以估算一个特征的重要程度。

## 提升法（boosting）

- 提升法（Boosting，假设提升）：是指可以将几个弱学习器结合成一个强学习器的任意集成方法。
- 大多数boosting的总体思想是循环训练预测器，每一次都对其前序做出一些改正。
- 可用的提升法有很多，但目前最流行的方法是**AdaBoost（自适应提升法，Adaptive Boosting）**和**梯度提升**。

### AdaBoost

- 新预测器对其前序进行纠正的办法之一，就是更多的关注前序拟合不足的训练实例。从而使新的预测器不断地越来越专注于难缠的问题，这就是AdaBoost使用的技术。
- AdaBoost这种依序循环的学习技术跟梯度下降有一些异曲同工之处，差别只在于——**不再是调整单个预测器的参数使成本函数最小化，而是不断在集成中加入预测器，使模型越来越好**。

------

我的CSDN：https://blog.csdn.net/qq_21579045

我的博客园：https://www.cnblogs.com/lyjun/

我的Github：https://github.com/TinyHandsome

纸上得来终觉浅，绝知此事要躬行~

欢迎大家过来OB~

by 李英俊小朋友