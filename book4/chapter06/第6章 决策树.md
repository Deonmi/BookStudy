# 第6章 决策树

## 写在前面

**参考书**

《机器学习实战——基于Scikit-Learn和TensorFlow》

**工具**

python3.5.1，Jupyter Notebook, Pycharm

## 决策树的可视化

- 生成一个决策树模型tree_clf

- 导出成dot格式的文件

  ```python
  export_graphviz(
      tree_clf,
      out_file=("./iris_tree.dot"),
      feature_names=iris.feature_names[2:],
      class_names=iris.target_names,
      rounded=True,
      filled=True
  )
  ```

- dot转png

  `dot -Tpng iris_tree.dot -o iris_tree.png`

## 决策树的优势

- 需要的数据准备工作非常少。特别是，完全不需要进行特征缩放或集中。
- sklearn使用的时CART算法，该算法仅生成二叉树。但是其他算法，比如ID3生成的决策树，其节点可以拥有两个以上的子节点。
- CART时一种贪婪算法：从顶层开始搜索最优分裂，然后每层重复这个过程。贪婪算法通常会产生一个相当不错的解，但是不能保证是最优解。
- 预测复杂度：$O(log_2(m))$，与特征数量无关，所以即使是处理大型数据集，预测也很快。
- 训练复杂度：$O(n*mlog(m))$，对于小型训练集（几千个实例以内），sklearn可以通过数据预处理（设置presort=True）来加快训练，但是对于较大的训练集而言，可能会减慢训练的速度。

## 基尼不纯度还是信息熵

- gini不纯度 = 1 - 求和【(类别为k的训练实例的占比)^2】
- 信息熵 = - 求和【(类别为k的训练实例占比)*log(类别为k的训练实例占比)】
- 大多数情况下，它们并没有什么大的不同，产生的树都很相似。
- gini不纯度计算速度略微快一些。
- 基尼不纯度倾向于从树枝中分裂出最常见的类别。
- 信息熵倾向于生产更平衡的树。

## 正则化超参数

- max_depth：最大深度
- min_samples_split：分裂前节点必须有的最小样本数
- min_samples_leaf：叶节点必须有的最小样本数
- min_weight_fraction_leaf跟min_samples_leaf一样，但表现为加权实例总数的占比
- max_leaf_nodes：最大叶节点数量
- max_features：分裂每个节点评估的最大特征数量
- 增大超参数`min_*`或是减小`max_*`将使模型正则化

## 不稳定性

- 决策树青睐正交的决策边界（所有分裂都与轴线垂直），这导致它们对训练集的旋转非常敏感。
- 更概括的说，决策树的主要问题是它们对训练数据中的小变化非常敏感。
- 限制这种问题的方法之一是使用PCA。
- 随机森林也可以限制这种不稳定性。

## 习题摘抄

- 如果训练集有100万个实例，训练决策树（无约束）大致的深度是多少？

  一个包含m个叶节点的均衡二叉树的深度等于$log_2m$的四舍五入。通常来说，二元决策树训练到最后大体都是平衡的。如果不加以限制，最后平均每个叶节点一个实例。因此，如果训练集包含一百万个实例，那么决策树深度约等于$log_2(10^6)\approx 20$层。实际上，会更多一些，因为决策树通常不可能完美平衡。

- 通常来说，子节点的基尼不纯度是高于还是低于父节点？是通常更高/更低？还是永远更高/更低？

  一个节点的基尼不纯度通常比其父节点低。这是通过CART训练算法的成本函数确保的。该算法分裂每个节点的方法，是使其子节点的基尼不纯度的加权之和最小。但是，如果一个子节点的不纯度远小于另一个，那么也有可能使子节点的基尼不纯度比其父节点高，只要那个不纯度更低的子节点能够抵偿这个增加即可。

  举例：

  假设一个节点包含4个A类别的实例和1个B类别的实例，其基尼不纯度等于$1-\frac15^2-\frac45^2=0.32$。现在我们假设数据使一维的，并且实例的排列顺序如下：A，B，A，A，A。你可以验证，算法将在第二个实例后拆分该节点，从而生成两个子节点所包含的实例分别为A，B和A，A，A。第一个子节点的基尼不纯度为$1-\frac12^2-\frac12^2=0.5$，比其父节点要高。这是因为第二个子节点是纯的，所以总的加权基尼不纯度等于$\frac25*0.5+\frac35*0.5=0.2$低于父节点的基尼不纯度。

- 如果在包含100万个实例的训练集上训练决策树需要一个小时，那么在包含1000万个实例的训练集上训练决策树，大概需要多长时间？

  决策树的训练复杂度为$O(n*mlog(m))$。所以，如果将训练集大小乘以10，训练时间将乘以

  $K=(n \times 10m \times log(10m)) / (n \times m \times log(m))=10 \times log(10m) / log(m)$

  如果$m=10^6$，那么$K \approx 11.7$，所以训练1000万个实例大约需要11.7小时。

- 如果训练集包含100000个实例，设置presort=True可以加快训练么？

  只有当数据集小于数千个实例时，预处理训练集才可以加速训练。如果包含100000个实例，设置presort=True会显著减慢训练。

## scipy.stats.mode

- 在Python中，我们可以用scipy.stats.mode函数寻找数组或者矩阵每行/每列中最常出现成员以及出现的次数 。
- 参考链接：[mode](https://blog.csdn.net/kane7csdn/article/details/84795405)

------

我的CSDN：https://blog.csdn.net/qq_21579045

我的博客园：https://www.cnblogs.com/lyjun/

我的Github：https://github.com/TinyHandsome

纸上得来终觉浅，绝知此事要躬行~

欢迎大家过来OB~

by 李英俊小朋友