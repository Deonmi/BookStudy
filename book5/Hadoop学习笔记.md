# Hadoop学习笔记

## - 视频笔记

视频教程连接：[大数据 *Hadoop* 视频教程全集](https://www.bilibili.com/video/av48621769?from=search&seid=4231285428491059279)

### 1. 大数据概论

- 大数据：海量的数据的存储和海量数据的分析计算问题。

- 大数据的特点（4V）：Volume（大量），Velocity（高速），Variety（多样），Value（低价值密度）。
- Hadoop的优势（4高）：高可靠性，高扩展性，高效性，高容错性。
-  Hadoop组成中1.x和2.x的区别：1.x中MapReduce负责计算和资源调度；在2.x中增加了Yarn，Yarn只负责资源的调度，MapReduce只负责运算。
- HDFS组成部分：
  - NameNode（nn）：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等。
  - DataNode（dn）：在本地文件系统存储文件块数据，以及块数据的校验和。
  - Secondary NameNode（2nn）：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照。

- Yarn组成部分：
  - ResourceManager（RM）：
    - 处理客户端请求
    - 监控NodeManager
    - 启动或监控ApplicationMaster
    - 资源的分配和调度
  - NodeManager（NM）：
    - 管理单个节点上的资源
    - 处理来自ResourceManager的命令
    - 处理来自ApplicationMaster的命令
  - ApplicationMaster（AM）：
    - 负责数据的切分
    - 为应用程序申请资源并分配给内部任务
    - 任务的监控与容错
  - Container：
    - Container是Yarn中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。



## - 《Hadoop权威指南》笔记

### 第一章 初识Hadoop

- 要对多个硬盘中的数据并行进行读写数据，需要解决的问题：
  1. 硬件故障问题
  2. 大多数分析任务需要以某种方式结合大部分数据来共同完成分析。

- **MapReduce**提出一个编程模型，该模型抽象出这些硬盘读写问题并将其转换为对一个数据集（由键-值对组成）的计算。

- 为什么不能用配有大量硬盘的数据库来进行大规模数据分析？我们为什么需要Hadoop？

  这两个问题的答案来自于计算机硬盘的另一个发展趋势：**寻址时间的提升远远不敌于传输速率的提升**。寻址是将磁头移动到特定硬盘位置进行读/写操作的过程。它是导致硬盘操作延迟的主要原因，而传输速率取决于硬盘的带宽。

- MapReduce适合一次写入、多次读取数据的应用；关系数据库则更适合持续更新的数据集。

|          | 传统的关系型数据库 | MapReduce          |
| -------- | ------------------ | ------------------ |
| 数据大小 | GB                 | PB                 |
| 数据存取 | 交互式和批处理     | 批处理             |
| 更新     | 多次读/写          | 一次写入，多次读取 |
| 事务     | ACID               | 无                 |
| 结构     | 写时模式           | 读时模式           |
| 完整性   | 高                 | 低                 |
| 横向扩展 | 非线性的           | 线性的             |

- **数据本地化（data locality）**特性是Hadoop数据处理的核心，并因此而获得良好的性能。
- Hadoop通过**显示网络拓扑结构**来保留网络带宽。
- MapReduce采用的是**无共享（shared-nothing）框架**，各个任务之间是彼此独立的。
- MapReduce的三大设计目标：
  - 为只需要短短几分钟或几个小时就可以完成的作业提供服务；
  - 运行于同一个内部有高速网络连接的数据中心内；
  - 数据中心内的计算机都是可靠的、专门的硬件。

