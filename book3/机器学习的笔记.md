# 机器学习的笔记

## 0. 写在前面

**参考书**

《Python数据科学手册》第五章“机器学习”

**工具**

Jupyter Lab

**作用**

给书中没有的知识点做补充。

## 1. 判定系数

1. 定义

判定系数（coefficient of determination），也叫可决系数或决定系数，是指在线性回归中，回归平方和与总离差平方和之比值，其数值等于相关系数的平方。它是对估计的回归方程拟合优度的度量。（参考：[百度百科](https://baike.baidu.com/item/%E5%88%A4%E5%AE%9A%E7%B3%BB%E6%95%B0/2393145?fr=aladdin)）

判定系数（记为R$^2$）在统计学中用于度量因变量的变异中可由自变量解释部分所占的比例，以此来判断统计模型的解释力。对于简单线性回归而言，判定系数为样本相关系数的平方。

假设一数据集包括$y_1, y_2, ..., y_n$共n个观察值，相对应的模型预测值分别为$f_1, f_2, ..., f_n$。定义残差$e_i = y_i - f_i$，

平均观察值为：$\bar{y} = \cfrac{1}{n} \sum\limits_{i=1}^n y_i$

于是可以得到总平方和：$SS_{tot} = \sum\limits_{i=1} (y_i - \bar{y})^2$

回归平方和：$SS_{reg} = \sum\limits_{i=1} (f_i - \bar{y})^2$

残差平方和：$SS_{res} = \sum\limits_{i=1} (y_i - f_i)^2 = \sum\limits_{i=1} e_i^2$

由此，判定系数可定义为：$R^2 = 1 - \cfrac{SS_{res}}{SS_{tot}}$

2. 总结

R$^2$ = 1：表示模型与数据完全吻合。

R$^2$ = 0：表示模型不比简单取均值好。

R$^2$ < 0：表示模型性能很差。

3. 系数标准

判定系数只是说明列入模型的所有解释变量对因变量的联合的影响程度，不说明模型中单个解释变量的影响程度。
判定系数达到多少为宜？没有一个统一的明确界限值；若建模的目的是预测因变量值，一般需考虑有较高的判定系数。若建模的目的是结构分析，就不能只追求高的判定系数，而是要得到总体回归系数的可信任的估计量。**判定系数高并不一定说明每个回归系数都可信任。**

## 2. 贝叶斯定理

我们在生活中经常遇到这种情况：我们可以很容易直接得出P(A|B)，P(B|A)则很难直接得出，但我们更关心P(B|A)，贝叶斯定理就为我们打通从P(A|B)求得P(B|A)的道路。

$P(B|A) = \cfrac{P(A|B)P(B)}{P(A)}$

推导：$P(A, B) = P(B|A) * P(A) = P(A|B) * P(B)$

参考：[机器学习之贝叶斯（贝叶斯定理、贝叶斯网络、朴素贝叶斯）](<https://blog.csdn.net/weixin_42180810/article/details/81278326>)

朴素贝叶斯分类是一种十分简单的分类算法，叫它朴素贝叶斯分类是因为这种方法的思想真的很朴素。

朴素贝叶斯的思想基础是这样的：**对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。**通俗来说，就好比这么个道理，你在街上看到一个黑人，我问你你猜这哥们哪里来的，你十有八九猜非洲。为什么呢？因为黑人中非洲人的比率最高，当然人家也可能是美洲人或亚洲人，但在没有其它可用信息下，我们会选择条件概率最大的类别，这就是朴素贝叶斯的思想基础。

- 最容易理解的朴素贝叶斯分类器：**高斯朴素贝叶斯**，参考：[透彻理解高斯分布](https://baijiahao.baidu.com/s?id=1621087027738177317&wfr=spider&for=pc)
- 





------

我的CSDN：https://blog.csdn.net/qq_21579045

我的博客园：https://www.cnblogs.com/lyjun/

我的Github：https://github.com/TinyHandsome

纸上得来终觉浅，绝知此事要躬行~

欢迎大家过来OB~

by 李英俊小朋友