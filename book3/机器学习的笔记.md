# 机器学习的笔记

## 0. 写在前面

**参考书**

《Python数据科学手册》第五章“机器学习”

**工具**

Jupyter Lab

**作用**

给书中没有的知识点做补充。

## 1. 判定系数

1. 定义

判定系数（coefficient of determination），也叫可决系数或决定系数，是指在线性回归中，回归平方和与总离差平方和之比值，其数值等于相关系数的平方。它是对估计的回归方程拟合优度的度量。（参考：[百度百科](https://baike.baidu.com/item/%E5%88%A4%E5%AE%9A%E7%B3%BB%E6%95%B0/2393145?fr=aladdin)）

判定系数（记为R$^2$）在统计学中用于度量因变量的变异中可由自变量解释部分所占的比例，以此来判断统计模型的解释力。对于简单线性回归而言，判定系数为样本相关系数的平方。

假设一数据集包括$y_1, y_2, ..., y_n$共n个观察值，相对应的模型预测值分别为$f_1, f_2, ..., f_n$。定义残差$e_i = y_i - f_i$，

平均观察值为：$\bar{y} = \cfrac{1}{n} \sum\limits_{i=1}^n y_i$

于是可以得到总平方和：$SS_{tot} = \sum\limits_{i=1} (y_i - \bar{y})^2$

回归平方和：$SS_{reg} = \sum\limits_{i=1} (f_i - \bar{y})^2$

残差平方和：$SS_{res} = \sum\limits_{i=1} (y_i - f_i)^2 = \sum\limits_{i=1} e_i^2$

由此，判定系数可定义为：$R^2 = 1 - \cfrac{SS_{res}}{SS_{tot}}$

2. 总结

R$^2$ = 1：表示模型与数据完全吻合。

R$^2$ = 0：表示模型不比简单取均值好。

R$^2$ < 0：表示模型性能很差。

3. 系数标准

判定系数只是说明列入模型的所有解释变量对因变量的联合的影响程度，不说明模型中单个解释变量的影响程度。
判定系数达到多少为宜？没有一个统一的明确界限值；若建模的目的是预测因变量值，一般需考虑有较高的判定系数。若建模的目的是结构分析，就不能只追求高的判定系数，而是要得到总体回归系数的可信任的估计量。**判定系数高并不一定说明每个回归系数都可信任。**

## 2. 朴素贝叶斯

1. 贝叶斯定理

我们在生活中经常遇到这种情况：我们可以很容易直接得出P(A|B)，P(B|A)则很难直接得出，但我们更关心P(B|A)，贝叶斯定理就为我们打通从P(A|B)求得P(B|A)的道路。

$P(B|A) = \cfrac{P(A|B)P(B)}{P(A)}$

推导：$P(A, B) = P(B|A) * P(A) = P(A|B) * P(B)$

参考：[机器学习之贝叶斯（贝叶斯定理、贝叶斯网络、朴素贝叶斯）](<https://blog.csdn.net/weixin_42180810/article/details/81278326>)

朴素贝叶斯分类是一种十分简单的分类算法，叫它朴素贝叶斯分类是因为这种方法的思想真的很朴素。

朴素贝叶斯的思想基础是这样的：**对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。**通俗来说，就好比这么个道理，你在街上看到一个黑人，我问你你猜这哥们哪里来的，你十有八九猜非洲。为什么呢？因为黑人中非洲人的比率最高，当然人家也可能是美洲人或亚洲人，但在没有其它可用信息下，我们会选择条件概率最大的类别，这就是朴素贝叶斯的思想基础。

- 假设每个标签的数据都服从简单的高斯分布，最容易理解的朴素贝叶斯分类器：**高斯朴素贝叶斯**，参考：[透彻理解高斯分布](https://baijiahao.baidu.com/s?id=1621087027738177317&wfr=spider&for=pc)。
- 假设特征是由一个简单的多项式分布生成，适合用于描述出现次数或者出现次数比列的特征：**多项式朴素贝叶斯**，参考：[朴素贝叶斯的三个常用模型：高斯、多项式、伯努利](<https://blog.csdn.net/qq_27009517/article/details/80044431>)

2. 朴素贝叶斯的应用场景

由于朴素贝叶斯分类器对数据有严格的假设，因此它的训练效果通常比复杂模型的差。其优点主要体现在以下四个方面。

- 训练和预测的速度非常快
- 直接使用概率预测
- 通常很容易理解
- 可调参数（如果有的话）非常少。

朴素贝叶斯分类器非常适合用于以下应用场景：

- 假设分布函数与数据匹配（实际中很少见）。
- 各种类型的区分度很高，模型复杂度不重要。
- 非常高维度的数据，模型复杂度不重要。

## 3. 自举重采样方法

模式识别中每个方法的实现都是基于一个特定的数据样本集的，但是这个样本集只是所有可能的样本中的一次随机抽样，毕竟在我们的生活实际中存在着万物众生，多到我们数也数不清，甚至计算机都无法统计的清，而我们搜集到的充其量只是其中很小很小的一部分，这也是为什么机器学习中缺少的只是数据，只要有足够多的学习数据，就一定能取得惊人的结果，因此模式识别和机器学习中的很多方法的实现结果都无疑会受到这种随机性的影响，我们训练得到的分类器也因此具有偶然性，尤其是样本不足够多时更为明显。

对于**决策树**而言，其树的生长是采用的**贪心算法**，只考虑**当前局部的最优**，因此其受这种随机性影响会更加严重，这也是为什么有的**决策树泛化能力那么差**的原因。

针对这种随机性的影响，最早在统计学中有人提出了一种叫做”自举（Bootstrap）“的策略，基本思想是对现有样本进行重复采样而产生多个样本子集，通过这种多次重复采样来模拟数据的随机性，然后在最后的输出结果中加进去这种随机性的影响。随后有人把这种自举的思想运用到了模式识别中，衍生出了一系列的解决方法，像随机森林、Bagging、Adaboost等。







------

我的CSDN：https://blog.csdn.net/qq_21579045

我的博客园：https://www.cnblogs.com/lyjun/

我的Github：https://github.com/TinyHandsome

纸上得来终觉浅，绝知此事要躬行~

欢迎大家过来OB~

by 李英俊小朋友